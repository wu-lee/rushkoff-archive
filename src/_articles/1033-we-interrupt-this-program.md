---
title: We Interrupt This Program
subtitle: 
date: 2011-02-01T05:00:00.000+00:00
publication: _publications/school_library_journal.md
blurb: 
notes: Date from [https://www.slj.com/story/we-interrupt-this-program-media-theorist-douglas-rushkoff-has-second-thoughts-about-our-digital-practices](https://www.slj.com/story/we-interrupt-this-program-media-theorist-douglas-rushkoff-has-second-thoughts-about-our-digital-practices
  "https://www.slj.com/story/we-interrupt-this-program-media-theorist-douglas-rushkoff-has-second-thoughts-about-our-digital-practices")
refs: 
sources: 
categories:
- _categories/journalism.md

---
Ask any kid what Facebook is for and he'll tell you it's there to help him make friends. What else could he think? It's how he does make friends. He has no idea that the real purpose of the software, and the people coding it, is to monetize his relationships. He isn't even aware of those people, the program, or their purpose.

The kids I celebrated in my early books as "digital natives," capable of seeing through all efforts of big media and marketing, have actually proven less able to discern the integrity of the sources they read and the intentions of the programs they use than we struggling adults are. If they don't know what the programs they're using are even for, they don't stand a chance at using them effectively. They're less likely to become power users than the used. It is our job as educators to change all this. We're our students' best chance of becoming media--or new media--literate. Yet our digital practices betray our own unconscious approach toward these media. We employ technologies in our lives and our curriculums by force of habit or fear of being left behind.

I regularly visit one-to-one laptop schools where neither the students nor the educators have any real sense of purpose about the highly technologized program they've implemented. They bring a very powerful new medium into the classroom and make it central without having reckoned with the medium's biases.

In just one example, I visited a college in Tennessee that had just reworked its "Model United Nations" class for the cyber era. For close to 20 years, students had conducted their simulated General Assembly meetings together in a room. They spent the semester learning about the cultures and political landscapes they were representing and then came together to negotiate face to face on their nations' behalf. In the new version of the class, students entered the very same room--now outfitted with long tables with computer terminals. Each student signed onto his or her own workstation, logged into a simulation called Second Life, and interacted in a virtual reality version of the classroom. They then conducted their same model UN as a cyber simulation.

The reason this should seem so silly to us is that these students were already in the same room. While a virtual classroom may be a great solution for distance learning, it's an unnecessary contrivance for those who are in the same place at the same time. That's one of the most expensive things about education: getting the people to the same place, keeping them warm, and so on. It's why students pay for dormitories, why school systems pay for maintenance workers, etc. The virtual United Nations class was working against one of the primary biases of digital media--its tendency to do things from a distance.

Likewise, our students' increasing dependence on digital communications technologies has vastly limited the real-world social cues on which we all depend for a sense of agreement, reinforcement, and collaboration. The mirror neurons in our brains are activated by the sight of someone else nodding at what we say, their pupils dilating, or their breathing synchronizing with our own. This allows us to read social cues, discern others' intents, develop empathy, and even model behaviors. It's how we learn from other human beings.

In virtual environments, we may have more access to raw data, but are denied this social reality. Only seven percent of communication takes place on the literal level allowed by most online interfaces. The other 93 percent takes place nonverbally. Incapable of transmitting this other 93 percent, our interactions online become highly literalized, suspicious, devoid of context, and continually parsed for their real meaning. We end up experiencing one another much in the way someone with Asperger's does.

## Answers without meaning

Meanwhile, the ease with which students can "cherry pick" knowledge online divorces ideas from their historical and intellectual context. Why read Romeo and Juliet when you can get the gist of it by scanning a one-paragraph summary on Sparknotes.com or an "in-depth" analysis on Wikipedia? Freeing one from the laborious effort of exploration or following a line of inquiry, a Google search is a casting out into the sea of data. Like shopping for an item on Amazon, everything except the "answer" can be ignored. Making matters worse, computer advocates are much too ready, to equate this with values such as democracy and intellectual freedom. Traditional disciplines become understood as the unnecessary divisions between knowledge that knows no such boundaries, rather than the historical project of coming to understand our world. Inquiry begins and ends with a single search, as getting the answer-particularly in an academic culture overrun by testing--takes precedence over learning.

There are ways to employ these technologies without succumbing to their worst biases. Even if we aren't going to join the rest of the developed world and teach programming to ourselves and our students, we can still come to terms with the main biases of these technologies and use them appropriately rather than automatically. In other words, just like anything else in our work and our lives, we can consciously choose when to use them, rather than feeling forced to use the same tool for every situation.

And as we do, we'll come to recognize the biases of these media and how to work with them. Teachers who may have once based their authority in their exclusive access to the knowledge of a particular discipline will now be confronted by students who can freely access more facts than the teacher even knows. Rather than feeling threatened, the best teachers will see this as an opportunity to move to the next level and understand their authority differently. They're not merely conveyors of data, but conveyors of meaning. They're now free to help students connect these data points, make sense, and develop context. This goes to the heart of what media specialists do. And these are the kinds of insights that occur between people in the real world, not between avatars on a server.

Likewise, computers and digital technology must be taught for their own sake. Digital tools are not like rakes, steam engines, or even automobiles that we can use with little understanding of how they work. Digital technology doesn't merely convey our bodies, but ourselves. Our screens are the windows through which we experience, organize, and interpret the world in which we live. We are doing more than extending human agency through a new linguistic or communications system. We are replicating the very function of cognition with external, extra-human mechanisms. These tools are not mere extensions of the will of some individual or group, but entities that have the ability to think and operate other components in the neural network--namely, us.

## Our technology, ourselves

And while machines once replaced and usurped the value of human labor, computers and networks do more than usurp the value of human thought. They not only copy our intellectual processes--our repeatable programs--but they often discourage our more complex processes--our higher-order cognition, contemplation, innovation, and meaning making that should be the reward of "ontsourcing" our arithmetic to silicon chips in the first place. The more humans become involved in their design, the more humanely inspired these tools will end up behaving.

I've been a computer enthusiast since the late '70s, and I believe that this is the moment we've been waiting for. We're gaining the ability to consciously participate in our evolution as a species. We are networking ourselves together into something perhaps greater than the sum of our many parts. But we must not relinquish our participation in this project, entrusting our future to the few who learn to program or the companies paying them to do so.

As we come to experience more of our world and one another through our digital interfaces, programming amounts to basic literacy. Even if we can't truly program ourselves, recognizing how the programs we use really work is revolutionary in itself. For once people come to see the way their technologies are programmed, they start to recognize the programs at play everywhere else--from the economy and education to politics and government.

All systems have embedded purposes. The less we recognize them, the more we mistake them for given circumstances. We start to treat the map as the territory. At the very least, we must come to recognize and teach the biases--the tendencies--of the technologies we are using, and encourage our young people to do the same. If we don't participate in building our digital future together, it will be done by someone--or something--else.

Amazingly, America--the birthplace of the Internet--is one of the only developed nations that doesn't teach programming in its public schools. Sure, some of our schools have elected to offer "computer" classes, but instead of teaching programming, these classes almost invariably teach programs: how to use Microsoft Office, Adobe Photoshop, or any of the other commercial software packages used in the average workplace. We teach our kids how to get jobs in today's marketplace rather than how to innovate for tomorrow's.

I believe this is a great mistake. What we think of as "literacy" must be redefined every time a new medium emerges. Literacy once meant the ability to read and write text. Now it's the ability to read and write programs. Unfortunately, however, when a new medium emerges, we generally only seek the capability offered by the one before it. The emergence of text did not lead to a world of readers, but one of listeners, who gathered at the town square to hear the Torah or Bible read to them by a rabbi or priest of the elite. Likewise, the invention of the printing press did not lead to a civilization of writers, but one of readers. Use of the press was reserved, by force, to an approved elite.

## Program or be programmed

Today, we have computers. Yet instead of teaching our students how to program them, we're content to teach them how to write with them. While we might celebrate their newfound access to publishing and video distribution, these are really just the skills of the last media revolution. Those seizing the real power of this medium have moved on.

It's like teaching kids how to listen, but not to speak; how to read, but not how to write. They learn to use programs, but not how to use computers. As a result, they're not technology's true users, but the used.

Computers, properly understood, are "anything" machines. They're as blank and malleable as a pad of paper. But we present them to our students the way we show them books--as finished objects, closed to intervention. Google, Facebook, and the Wii, for that matter, seem to them preexisting conditions, rather than interfaces designed by particular people with particular goals.

Just last year, while researching a book on America's digital illiteracy, I met with the Air Force general then in charge of America's cybercommand. He said he had plenty of new recruits ready and able to operate drones or other virtual fighting machines--but no one capable of programming them, or even interested in learning how. He wasn't even getting recruits who were ready to begin basic programming classes. Meanwhile, he explained to me, colleges in Russia, China, and even Iran were churning out an order of magnitude more programmers than universities in the U.S. It's only a matter of time, he said--a generation at most--until our military loses its digital superiority.

If we continue to treat programming as a menial skill to be outsourced to developing nations, we'll lose our innovative superiority as well. While this may not hurt American corporations capable of sourcing their code from anywhere, it would certainly hurt Americans looking for a skill set to replace our manufacturing jobs.

But these are just arguments for us to make to our funders. In a deeper sense, our inability and refusal to contend with the underlying biases of the programs and networks we all use is less a threat to our military or economic superiority than to our experience and autonomy as people. I can't think of a time when we seemed so ready to accept such a passive relationship to a medium or technology.

New media require new literacies. We must work to become literate ourselves if we expect to pass the value of literacy on to our students.

*Douglas Rushkoff (rushkoff.com) is the author, most recently, of Program or Be Programmed: Ten Commands for a Digital Age (OR Books, 2010).*